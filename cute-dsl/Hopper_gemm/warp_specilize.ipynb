{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9e5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "import math\n",
    "import operator\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import cutlass\n",
    "import cutlass.cute as cute\n",
    "from cutlass import const_expr, Float16, Float32, Int32\n",
    "from cutlass.cute.nvgpu import cpasync, warp, warpgroup\n",
    "from cutlass.cutlass_dsl import T, dsl_user_op\n",
    "from cutlass._mlir.dialects import llvm, nvvm, vector\n",
    "from cutlass.utils import LayoutEnum, StaticPersistentTileScheduler\n",
    "import cutlass.utils.hopper_helpers as sm90_utils\n",
    "from cutlass.cute.runtime import from_dlpack\n",
    "import cuda.bindings.driver as cuda\n",
    "from cutlass.cute.runtime import make_ptr\n",
    "import cutlass.pipeline as pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4382a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.jit\n",
    "def tile_scheduler_get_next(cur_id, group_size, group_size_m, max_m):\n",
    "    group_id = cur_id // group_size\n",
    "    id_in_group = cur_id % group_size\n",
    "    first_pid_m = group_id * group_size_m\n",
    "    real_group_m = min(max_m - first_pid_m, group_size_m)\n",
    "    row = id_in_group % real_group_m\n",
    "    col = id_in_group // real_group_m\n",
    "    return first_pid_m+row, col\n",
    "\n",
    "@cute.kernel\n",
    "def gemm_kernel(\n",
    "    tma_atom_a: cute.CopyAtom,\n",
    "    mA: cute.Tensor,\n",
    "    tma_atom_b: cute.CopyAtom,\n",
    "    mB: cute.Tensor,\n",
    "    tma_atom_c: cute.CopyAtom,\n",
    "    mC: cute.Tensor,\n",
    "    tiled_mma: cute.TiledMma,\n",
    "    a_smem_layout: cute.ComposedLayout,\n",
    "    b_smem_layout: cute.ComposedLayout,\n",
    "    c_smem_layout: cute.ComposedLayout,\n",
    "    g_mnk: cute.Shape,\n",
    "    b_mnk: cute.Shape,\n",
    "    NUM_SM: cutlass.Constexpr,\n",
    "    shared_storage: cutlass.Constexpr,\n",
    "):\n",
    "    tx, _, _ = cute.arch.thread_idx()\n",
    "    bx, _, _ = cute.arch.block_idx()\n",
    "    warp_idx = cute.arch.warp_idx()\n",
    "    warp_idx = cute.arch.make_warp_uniform(warp_idx)\n",
    "    if warp_idx == 0:\n",
    "        cute.nvgpu.cpasync.prefetch_descriptor(tma_atom_a)\n",
    "        cute.nvgpu.cpasync.prefetch_descriptor(tma_atom_b)\n",
    "        cute.nvgpu.cpasync.prefetch_descriptor(tma_atom_c)\n",
    "    \n",
    "    cta_rank_in_cluster = cute.arch.make_warp_uniform(\n",
    "        cute.arch.block_idx_in_cluster()\n",
    "    )\n",
    "    M, N, K = g_mnk\n",
    "    BM, BN, BK = b_mnk\n",
    "    # for tile swizzle\n",
    "    tiles_m = (M+BM-1) // BM\n",
    "    tiles_n = (N+BN-1) // BN\n",
    "    num_tiles = tiles_m * tiles_n\n",
    "    print(f\"tiles_m={tiles_m}\")\n",
    "    print(f\"tiles_n={tiles_n}\")\n",
    "    print(f\"num_tiles={num_tiles}\")\n",
    "    group_size_m = 8\n",
    "    group_size = group_size_m * tiles_n\n",
    "    \n",
    "    stages = cute.product(a_smem_layout.shape[2])\n",
    "    gA = cute.local_tile(\n",
    "        mA, (BM, BK), (None,None)\n",
    "    ) # (128,64,ktiles):(1@1,1@0,64@0)\n",
    "    gB = cute.local_tile(\n",
    "        mB, (BN, BK), (None,None)\n",
    "    ) # (256,64,ktiles):(1@0,1@1,64@1)>\n",
    "    gC = cute.local_tile(\n",
    "        mC, (BM, BN), (None,None)\n",
    "    ) # (128,256):(4096,1)\n",
    "    print(f\"gA={gA}\")\n",
    "    print(f\"gB={gB}\")\n",
    "    print(f\"gC={gC}\")\n",
    "\n",
    "    smem = cutlass.utils.SmemAllocator()\n",
    "    storage = smem.allocate(shared_storage)\n",
    "    sA = storage.sA.get_tensor(a_smem_layout.outer, swizzle=a_smem_layout.inner)\n",
    "    sB = storage.sB.get_tensor(b_smem_layout.outer, swizzle=b_smem_layout.inner)\n",
    "    sC = storage.sC.get_tensor(c_smem_layout.outer, swizzle=c_smem_layout.inner)\n",
    "    sA_tma = cute.group_modes(sA, 0, 2)\n",
    "    gA_tma = cute.group_modes(gA, 0, 2)\n",
    "    tAsA, tAgA = cute.nvgpu.cpasync.tma_partition(\n",
    "        tma_atom_a,\n",
    "        cta_coord=0,\n",
    "        cta_layout=cute.make_layout((1,)),\n",
    "        smem_tensor=sA_tma,\n",
    "        gmem_tensor=gA_tma,\n",
    "    )\n",
    "    sB_tma = cute.group_modes(sB, 0, 2)\n",
    "    gB_tma = cute.group_modes(gB, 0, 2)\n",
    "    tBsB, tBgB = cute.nvgpu.cpasync.tma_partition(\n",
    "        tma_atom_b,\n",
    "        cta_coord=cta_rank_in_cluster,\n",
    "        cta_layout=cute.make_layout((2,)),\n",
    "        smem_tensor=sB_tma,\n",
    "        gmem_tensor=gB_tma,\n",
    "    )\n",
    "    tma_sC, tma_gC = cute.nvgpu.cpasync.tma_partition(\n",
    "        tma_atom_c,\n",
    "        cta_coord=0,\n",
    "        cta_layout=cute.make_layout((1,)),\n",
    "        smem_tensor=cute.group_modes(sC, 0, 2),\n",
    "        gmem_tensor=cute.group_modes(gC, 0, 2),\n",
    "    )\n",
    "    barrier_ptr = storage.barrier_ptr.data_ptr()\n",
    "    empty_ptr = barrier_ptr+stages\n",
    "    with cute.arch.elect_one():\n",
    "        for i in cutlass.range_constexpr(stages):\n",
    "            cute.arch.mbarrier_init(barrier_ptr+i, cnt=1)\n",
    "            cute.arch.mbarrier_init(empty_ptr+i, cnt=2*2) # two consumer\n",
    "    cute.arch.mbarrier_init_fence()\n",
    "    cute.arch.cluster_arrive_relaxed()\n",
    "    \n",
    "    tma_copy_bytes = cute.size_in_bytes(Float16, cute.slice_(a_smem_layout, (None, None, 0))) \\\n",
    "        + cute.size_in_bytes(Float16, cute.slice_(b_smem_layout, (None, None, 0)))\n",
    "    cute.arch.cluster_wait()\n",
    "    is_producer = warp_idx < 4\n",
    "    if is_producer:\n",
    "        cute.arch.warpgroup_reg_dealloc(24)\n",
    "        if warp_idx == 0:\n",
    "            phase = 1\n",
    "            smem_k = 0\n",
    "            tile_id = bx\n",
    "            while tile_id < num_tiles:\n",
    "                bm, bn = tile_scheduler_get_next(tile_id, group_size, group_size_m, tiles_m)\n",
    "                tile_id += NUM_SM\n",
    "                # cute.printf(\"tile_id={} bm={} bn={}\", tile_id, bm, bn)\n",
    "                for k in range(gA.shape[3]):\n",
    "                    if smem_k == stages:\n",
    "                        smem_k = 0\n",
    "                        phase ^= 1\n",
    "                    cute.arch.mbarrier_wait(empty_ptr+smem_k, phase)\n",
    "                    cur_barrier = barrier_ptr+smem_k\n",
    "                    with cute.arch.elect_one():\n",
    "                        cute.arch.mbarrier_arrive_and_expect_tx(cur_barrier, tma_copy_bytes)\n",
    "                    cute.copy(\n",
    "                        tma_atom_a,\n",
    "                        tAgA[(None, bm, k)],\n",
    "                        tAsA[(None, smem_k)],\n",
    "                        tma_bar_ptr=cur_barrier,\n",
    "                    )\n",
    "                    cute.copy(\n",
    "                        tma_atom_b,\n",
    "                        tBgB[(None, bn, k)],\n",
    "                        tBsB[(None, smem_k)],\n",
    "                        tma_bar_ptr=cur_barrier,\n",
    "                        mcast_mask=cutlass.Int16(3),\n",
    "                    )\n",
    "                    # if tx == 0 and bx == 0 and by == 0:\n",
    "                    #     cute.printf(\"PRODUCER: TMA copy issued. k={}\", k)\n",
    "                    smem_k += 1\n",
    "    else:\n",
    "        cute.arch.warpgroup_reg_alloc(240)\n",
    "        tid_in_wg = tx % 128\n",
    "        thr_mma = tiled_mma.get_slice(tx-128)\n",
    "        tCsA = thr_mma.partition_A(sA)\n",
    "        tCsB = thr_mma.partition_B(sB)\n",
    "        tCgC = thr_mma.partition_C(gC)\n",
    "        tCsC = thr_mma.partition_C(sC)\n",
    "        tCrA = tiled_mma.make_fragment_A(tCsA)\n",
    "        tCrB = tiled_mma.make_fragment_B(tCsB)\n",
    "        acc_shape = tiled_mma.partition_shape_C(\n",
    "            (BM, BN)\n",
    "        )\n",
    "        tCrC = cute.make_fragment(acc_shape, Float32)\n",
    "        print(f\"tCgC={tCgC}\")\n",
    "        print(f\"tCsC={tCsC}\")\n",
    "        print(f\"tCrC={tCrC}\")\n",
    "        tiled_mma.set(cute.nvgpu.warpgroup.Field.ACCUMULATE, True)\n",
    "        phase = 0\n",
    "        smem_k = 0\n",
    "        # for tile_id in cutlass.range(bx, num_tiles, 78):\n",
    "        tile_id = bx\n",
    "        while tile_id < num_tiles:\n",
    "            tCrC.fill(0.0)\n",
    "            bm, bn = tile_scheduler_get_next(tile_id, group_size, group_size_m, tiles_m)\n",
    "            # if bx==0 and tx==128:\n",
    "            #     cute.printf(\"tile_id={}, group_size={}, group_size_m={} tiles_m={}\", tile_id, group_size, group_size_m, tiles_m)\n",
    "            #     cute.printf(\"tile_id={}, bm={}, bn={} num_tiles={}\", tile_id, bm, bn, num_tiles)\n",
    "            tile_id += NUM_SM\n",
    "            for k in range(gA.shape[3]):\n",
    "                if smem_k == 3:\n",
    "                    smem_k = 0\n",
    "                    phase ^= 1\n",
    "                cur_barrier = barrier_ptr + smem_k\n",
    "                cute.arch.mbarrier_wait(cur_barrier, phase)\n",
    "                cur_stage_coord = (\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,      # tile_mma k\n",
    "                    smem_k,    # pipeline stage\n",
    "                )\n",
    "                cute.nvgpu.warpgroup.fence()\n",
    "                cute.gemm(\n",
    "                    tiled_mma,\n",
    "                    tCrC,\n",
    "                    tCrA[cur_stage_coord],\n",
    "                    tCrB[cur_stage_coord],\n",
    "                    tCrC,\n",
    "                )\n",
    "                cute.nvgpu.warpgroup.commit_group()\n",
    "                cute.nvgpu.warpgroup.wait_group(0)\n",
    "                if tid_in_wg < 2:\n",
    "                    cute.arch.mbarrier_arrive(empty_ptr+smem_k, tid_in_wg)\n",
    "                smem_k += 1\n",
    "\n",
    "            tCrC_dtype = cute.make_fragment_like(tCrC, Float16)\n",
    "            tCrC_dtype.store(tCrC.load().to(Float16))\n",
    "            # cute.autovec_copy(tCrC_dtype, tCgC[(None,None,None, bm, bn)])\n",
    "            cute.arch.cp_async_wait_group(0)\n",
    "            cute.autovec_copy(tCrC_dtype, tCsC)\n",
    "            print(f\"tma_gC={tma_gC}\")\n",
    "            print(f\"tma_sC={tma_sC}\")\n",
    "            cute.arch.barrier(barrier_id=10, number_of_threads=256)\n",
    "            if tx == 128:\n",
    "                cute.copy(\n",
    "                    tma_atom_c,\n",
    "                    tma_sC,\n",
    "                    tma_gC[(None, bm, bn)],\n",
    "                )\n",
    "                cute.arch.cp_async_commit_group()\n",
    "\n",
    "        # if tx == 0 and bx==0 and by==0:\n",
    "        #     cute.printf(\"tCrC={}\", tCrC)\n",
    "        #     cute.printf(\"tCrC_D={}\", tCrC_dtype))\n",
    "        # c_copy_atom = cute.make_copy_atom(cute.nvgpu.CopyUniversalOp(), mC.element_type)\n",
    "        # cute.copy(c_copy_atom, tCrC_dtype, tCgC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459dc229",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.jit\n",
    "def gemm_tn(\n",
    "    a_ptr: cute.Pointer,\n",
    "    b_ptr: cute.Pointer,\n",
    "    c_ptr: cute.Pointer,\n",
    "    m: cutlass.Constexpr,\n",
    "    n: cutlass.Constexpr,\n",
    "    k: cutlass.Constexpr,\n",
    "    NUM_SM: cutlass.Constexpr,\n",
    "):\n",
    "    a_layout = cute.make_ordered_layout((m, k), order=(1, 0))\n",
    "    b_layout = cute.make_ordered_layout((n, k), order=(0, 1))\n",
    "    c_layout = cute.make_ordered_layout((m, n), order=(1, 0))\n",
    "    mA = cute.make_tensor(a_ptr, layout=a_layout)\n",
    "    mB = cute.make_tensor(b_ptr, layout=b_layout)\n",
    "    mC = cute.make_tensor(c_ptr, layout=c_layout)\n",
    "    a_dtype = a_ptr.dtype\n",
    "    b_dtype = b_ptr.dtype\n",
    "    a_smem_layout_atom = warpgroup.make_smem_layout_atom(\n",
    "        cute.nvgpu.warpgroup.SmemLayoutAtomKind.K_SW128,\n",
    "        a_dtype,\n",
    "    )\n",
    "    b_smem_layout_atom = warpgroup.make_smem_layout_atom(\n",
    "        cute.nvgpu.warpgroup.SmemLayoutAtomKind.MN_SW128,\n",
    "        b_dtype,\n",
    "    )\n",
    "    stages = 3\n",
    "    BM, BN, BK = 128, 256, 64\n",
    "    a_smem_layout = cute.tile_to_shape(\n",
    "        a_smem_layout_atom,\n",
    "        (BM, BK, stages),\n",
    "        order=(0, 1, 2),\n",
    "    )\n",
    "    b_smem_layout = cute.tile_to_shape(\n",
    "        b_smem_layout_atom,\n",
    "        (BN, BK, stages),\n",
    "        order=(1, 0, 2),\n",
    "    )\n",
    "    c_smem_layout = cute.tile_to_shape(\n",
    "        a_smem_layout_atom,\n",
    "        (BM, BN),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    tiled_mma = sm90_utils.make_trivial_tiled_mma(\n",
    "        a_dtype,\n",
    "        b_dtype,\n",
    "        LayoutEnum.ROW_MAJOR.sm90_mma_major_mode(),\n",
    "        LayoutEnum.COL_MAJOR.sm90_mma_major_mode(),\n",
    "        Float32,\n",
    "        atom_layout_mnk=(2,1,1),\n",
    "        tiler_mn=(64, BN),\n",
    "    )\n",
    "    buffer_align_bytes = 1024\n",
    "    @cute.struct\n",
    "    class SharedStorage:\n",
    "        barrier_ptr: cute.struct.MemRange[\n",
    "            cutlass.Int64, stages*2\n",
    "        ]\n",
    "        sA: cute.struct.Align[\n",
    "            cute.struct.MemRange[\n",
    "                a_dtype, cute.cosize(a_smem_layout)\n",
    "            ],\n",
    "            buffer_align_bytes,\n",
    "        ]\n",
    "        sB: cute.struct.Align[\n",
    "            cute.struct.MemRange[\n",
    "                b_dtype, cute.cosize(b_smem_layout)\n",
    "            ],\n",
    "            buffer_align_bytes,\n",
    "        ]\n",
    "        sC: cute.struct.Align[\n",
    "            cute.struct.MemRange[\n",
    "                a_dtype, cute.cosize(c_smem_layout)\n",
    "            ],\n",
    "            buffer_align_bytes,\n",
    "        ]\n",
    "    tma_atom_A, tma_tensor_A = cute.nvgpu.cpasync.make_tiled_tma_atom(\n",
    "        cute.nvgpu.cpasync.CopyBulkTensorTileG2SOp(),\n",
    "        mA,\n",
    "        cute.slice_(a_smem_layout, (None,None,0)),\n",
    "        (BM, BK),\n",
    "        num_multicast=1,\n",
    "    )\n",
    "    tma_atom_B, tma_tensor_B = cute.nvgpu.cpasync.make_tiled_tma_atom(\n",
    "        cute.nvgpu.cpasync.CopyBulkTensorTileG2SMulticastOp(),\n",
    "        mB,\n",
    "        cute.slice_(b_smem_layout, (None,None,0)),\n",
    "        (BN, BK),\n",
    "        num_multicast=2,\n",
    "    )\n",
    "    tma_atom_C, tma_tensor_C = cute.nvgpu.cpasync.make_tiled_tma_atom(\n",
    "        cute.nvgpu.cpasync.CopyBulkTensorTileS2GOp(),\n",
    "        mC,\n",
    "        c_smem_layout,\n",
    "        (BM, BN),\n",
    "        num_multicast=1,\n",
    "    )\n",
    "    gemm_kernel(\n",
    "        tma_atom_A, tma_tensor_A,\n",
    "        tma_atom_B, tma_tensor_B,\n",
    "        tma_atom_C, tma_tensor_C,\n",
    "        tiled_mma,\n",
    "        a_smem_layout,\n",
    "        b_smem_layout,\n",
    "        c_smem_layout,\n",
    "        (m, n, k),\n",
    "        (BM, BN, BK),\n",
    "        NUM_SM,\n",
    "        SharedStorage,\n",
    "    ).launch(\n",
    "        # grid=[1, 1, 1],\n",
    "        grid=[NUM_SM, 1, 1],\n",
    "        block=[128*3, 1, 1],\n",
    "        cluster=[2, 1, 1],\n",
    "        # smem=get_smem_size_bytes(tiler_mn, num_warps),\n",
    "        # no cluster for now\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070710a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiles_m=32\n",
      "tiles_n=16\n",
      "num_tiles=512\n",
      "gA=tensor<(0,0) o (128,64,32,64):(1@1,1@0,128@1,64@0)>\n",
      "gB=tensor<(0,0) o (256,64,16,64):(1@0,1@1,256@0,64@1)>\n",
      "gC=tensor<(0,0) o (128,256,32,16):(1@1,1@0,128@1,256@0)>\n",
      "tCgC=tensor<(?{div=2},?) o ((2,2,32),1,1,32,16):((1@0,8@1,8@0),0,0,128@1,256@0)>\n",
      "tCsC=tensor<ptr<f16, smem, align<4>, S<3,4,3>> o ((2,2,(8,4)),1,1):((1,512,(8,8192)),0,0)>\n",
      "tCrC=tensor<ptr<f32, rmem, align<32>> o ((2,2,32),1,1):((1,2,4),0,0)>\n",
      "tma_gC=tensor<(0,0) o (((64,128),4),32,16):(((1@0,1@1),64@0),128@1,256@0)>\n",
      "tma_sC=tensor<ptr<f16, smem, align<1024>, S<3,4,3>> o ((8192,4)):((1,8192))>\n",
      "cublas FLOPS: 754.517 TFLOPS\n",
      "cuteDSL FLOPS: 750.903 TFLOPS\n"
     ]
    }
   ],
   "source": [
    "props = torch.cuda.get_device_properties()\n",
    "multi_processor_count = props.multi_processor_count\n",
    "m, n, k = 4096, 4096, 4096\n",
    "torch.manual_seed(22)\n",
    "a = torch.randn(m, k, device=\"cuda\", dtype=torch.float16) / 64\n",
    "b = torch.randn(k, n, device=\"cuda\", dtype=torch.float16)\n",
    "c = torch.zeros(m, n, device=\"cuda\", dtype=torch.float16)\n",
    "a_ptr = make_ptr(\n",
    "    cutlass.Float16, a.data_ptr(), cute.AddressSpace.gmem, assumed_align=32\n",
    ")\n",
    "b_ptr = make_ptr(\n",
    "    cutlass.Float16, b.data_ptr(), cute.AddressSpace.gmem, assumed_align=32\n",
    ")\n",
    "c_ptr = make_ptr(\n",
    "    cutlass.Float16, c.data_ptr(), cute.AddressSpace.gmem, assumed_align=32\n",
    ")\n",
    "ref_c = a @ b\n",
    "\n",
    "compiled_gemm = cute.compile(gemm_tn, a_ptr, b_ptr, c_ptr, m,n,k,multi_processor_count)\n",
    "compiled_gemm(a_ptr, b_ptr, c_ptr)\n",
    "\n",
    "# torch.set_printoptions(sci_mode=False, threshold=float('inf'), linewidth=999999)\n",
    "torch.testing.assert_close(c, ref_c, atol=1e-2, rtol=1e-2)\n",
    "\n",
    "# warm up\n",
    "for i in range(32):\n",
    "    ref_c = a @ b\n",
    "\n",
    "def get_flops(func):\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    torch.cuda.synchronize()\n",
    "    for i in range(32):\n",
    "        ref_c = a @ b\n",
    "    start_event.record()\n",
    "    for _ in range(64):\n",
    "        func()\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    total_flop = m*n*k*2\n",
    "    time_ms = start_event.elapsed_time(end_event)\n",
    "    per_ms = time_ms / 64\n",
    "    return total_flop / (per_ms / 1000)\n",
    "\n",
    "print(f\"cublas FLOPS: {get_flops(lambda: torch.mm(a, b)) / 1e12:.3f} TFLOPS\")\n",
    "import time\n",
    "time.sleep(2)\n",
    "print(f\"cuteDSL FLOPS: {get_flops(lambda: compiled_gemm(a_ptr, b_ptr, c_ptr)) / 1e12:.3f} TFLOPS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
