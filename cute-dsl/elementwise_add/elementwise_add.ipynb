{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1637bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "import cutlass\n",
    "import cutlass.cute as cute\n",
    "from cutlass.cute.runtime import from_dlpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3234382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.kernel\n",
    "def naive_elementwise_add_kernel(\n",
    "    gA: cute.tensor,\n",
    "    gB: cute.tensor,\n",
    "    gC: cute.tensor,\n",
    "):\n",
    "    tidx, _, _ = cute.arch.thread_idx()\n",
    "    bidx, _, _ = cute.arch.block_idx()\n",
    "    bdimx, _, _ = cute.arch.block_dim()\n",
    "\n",
    "    thread_idx = bidx * bdimx + tidx\n",
    "\n",
    "    m, n = gA.shape\n",
    "    ni = thread_idx % n\n",
    "    mi = thread_idx // n\n",
    "\n",
    "    a_val = gA[mi, ni]\n",
    "    b_val = gB[mi, ni]\n",
    "\n",
    "    gC[mi, ni] = a_val + b_val\n",
    "\n",
    "\n",
    "@cute.jit\n",
    "def naive_elementwise_add(\n",
    "    mA: cute.Tensor,\n",
    "    mB: cute.Tensor,\n",
    "    mC: cute.Tensor,\n",
    "):  \n",
    "    num_threads_per_block = 256\n",
    "\n",
    "    m, n = mA.shape\n",
    "\n",
    "    kernel = naive_elementwise_add_kernel(mA, mB, mC)\n",
    "    grid = (m * n + num_threads_per_block - 1) // num_threads_per_block\n",
    "    kernel.launch(grid=grid,\n",
    "                  block=(num_threads_per_block, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd14cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#M, N = 4096, 4096\n",
    "M, N = 40960, 8192\n",
    "\n",
    "a = torch.randn(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "b = torch.randn(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "c = torch.randn(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "a_ = from_dlpack(a, assumed_align=16)\n",
    "b_ = from_dlpack(b, assumed_align=16)\n",
    "c_ = from_dlpack(c, assumed_align=16)\n",
    "\n",
    "naive_elementwise_add_ = cute.compile(naive_elementwise_add, a_, b_, c_)\n",
    "naive_elementwise_add_(a_, b_, c_)\n",
    "\n",
    "torch.testing.assert_close(c, a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ce0b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time: 1.1835ms\n",
      "Performance (GFLOPS): 283.5213GFLOPS\n",
      "Effective Memory Bandwidth: 3402.26 GB/s\n"
     ]
    }
   ],
   "source": [
    "def benchmark(callable, *, num_warmups, num_iters):\n",
    "\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    for _ in range(num_warmups):\n",
    "        callable()\n",
    "\n",
    "    start_event.record(stream=torch.cuda.current_stream())\n",
    "    for _ in range(num_iters):\n",
    "        callable()\n",
    "    end_event.record(stream=torch.cuda.current_stream())\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time = start_event.elapsed_time(end_event)\n",
    "    avg_time = elapsed_time / num_iters\n",
    "    gflops = a.numel() / (avg_time / 1000) / 1e9\n",
    "\n",
    "    print(f\"Average execution time: {avg_time:.4f}ms\")\n",
    "    print(f\"Performance (GFLOPS): {gflops:.4f}GFLOPS\")\n",
    "    print(f\"Effective Memory Bandwidth: {(3 * a.numel() * 4) / (avg_time / 1000) / 1e9:.2f} GB/s\")\n",
    "\n",
    "benchmark(partial(naive_elementwise_add_, a_, b_, c_), num_warmups=5, num_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da825f59",
   "metadata": {},
   "source": [
    "### 向量化的LD/ST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c00032",
   "metadata": {},
   "source": [
    "#### cute_divide_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e41802b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape torch.Size([16, 32]) Stride: (32, 1)\n",
      "[DSL INFO] Tiled Tensors:\n",
      "[DSL INFO] Tiler: (2, 4)\n",
      "[DSL INFO] logical_divide  lA = tensor<ptr<f32, gmem, align<16>> o ((2,8),(4,8)):((32,64),(1,4))>\n",
      "====================================================================================================\n",
      "[DSL INFO] zipped_divide  zA = tensor<ptr<f32, gmem, align<16>> o ((2,4),(8,8)):((32,1),(64,4))>\n",
      "[DSL INFO] flatten_divide  fA = tensor<ptr<f32, gmem, align<16>> o (2,4,8,8):(32,1,64,4)>\n",
      "[DSL INFO] tiled_divide  tA = tensor<ptr<f32, gmem, align<16>> o ((2,4),8,8):((32,1),64,4)>\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def devide_layout_test(\n",
    "    mA: cute.Tensor\n",
    "):\n",
    "    tiler = (2, 4)\n",
    "    lA = cute.logical_divide(mA, tiler=tiler)\n",
    "    zA = cute.zipped_divide(mA, tiler=tiler)\n",
    "    fA = cute.flat_divide(mA, tiler=tiler)\n",
    "    tA = cute.tiled_divide(mA, tiler=tiler)\n",
    "    \n",
    "    print(f\"[DSL INFO] Tiled Tensors:\")\n",
    "    print(f\"[DSL INFO] Tiler: {tiler}\")\n",
    "    print(f\"[DSL INFO] logical_divide  lA = {lA}\")    \n",
    "    print(100*'=')\n",
    "    print(f\"[DSL INFO] zipped_divide  zA = {zA}\")\n",
    "    print(f\"[DSL INFO] flatten_divide  fA = {fA}\")\n",
    "    print(f\"[DSL INFO] tiled_divide  tA = {tA}\")\n",
    "\n",
    "M, N = 16, 32\n",
    "a = torch.randn(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "print(f\"Tensor shape {a.shape} Stride: {a.stride()}\")\n",
    "a_ = from_dlpack(a, assumed_align=16)\n",
    "devide_layout_test(a_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.kernel\n",
    "def vector_elementwise_add_kernel(\n",
    "    gA: cute.Tensor,\n",
    "    gB: cute.Tensor,\n",
    "    gC: cute.Tensor,\n",
    "):\n",
    "    tidx, _, _ = cute.arch.thread_idx()\n",
    "    bidx, _, _ = cute.arch.block_idx()\n",
    "    bdimx, _, _ = cute.arch.block_dim()\\\n",
    "    \n",
    "    thread_idx = bidx * bdimx + tidx\n",
    "#*                                   gA = cute.zipped_divide(mA, (1, 4))\n",
    "#*\n",
    "#*        Tile的Shape                         Thread的shape                          Tile的Stride\n",
    "#*   ((   [  1 ]  [  4 ]   ))            (   [ 4096 ] [ 1024 ]  )  :            ((   [  0 ]  [  1 ]   ))   ( [ 4096 ] [  4 ] )\n",
    "#*          ↑                                 ↑        ↑\n",
    "#*        None                               mi       ni\n",
    "#*\n",
    "    m, n = gA.shape[1]\n",
    "    print(f\"THread domain m={m} , n={n}\")\n",
    "    ni = thread_idx % n\n",
    "    mi = thread_idx // n\n",
    "\n",
    "#*                                   gA = gA[(None, (mi, ni))]\n",
    "#*                                                    ↓\n",
    "#*                                       ((   [  1 ]  [  4 ]   ), (   [  0 ]  [  1 ]   ))\n",
    "#*\n",
    "    a_val = gA[(None, (mi, ni))].load()\n",
    "    b_val = gB[(None, (mi, ni))].load()\n",
    "\n",
    "    print(f\"[DSL INFO] sliced gA = {gA[(None, (mi, ni))]}\")\n",
    "    print(f\"[DSL INFO] sliced gB = {gB[(None, (mi, ni))]}\")\n",
    "\n",
    "    gC[(None), (mi, ni)] = a_val + b_val\n",
    "\n",
    "\n",
    "@cute.jit\n",
    "def vectorized_elementwise_add(\n",
    "    mA: cute.Tensor,\n",
    "    mB: cute.Tensor,\n",
    "    mC: cute.Tensor\n",
    "):\n",
    "    threads_per_block = 256\n",
    "\n",
    "#*                                   gA = cute.zipped_divide(mA, (1, 4))\n",
    "#*\n",
    "#*       Tile的Shape                         Thread的shape                          Tile的Stride\n",
    "#*   ((   [  1 ]  [  4 ]   ))            (   [ 4096 ] [ 1024 ]  )  :            ((   [  0 ]  [  1 ]   ))   ( [ 4096 ] [  4 ] )\n",
    "#*          ↑                                 ↑        ↑\n",
    "#*        None                               mi       ni\n",
    "#*\n",
    "    gA = cute.zipped_divide(mA, (1, 4))\n",
    "    gB = cute.zipped_divide(mB, (1, 4))\n",
    "    gC = cute.zipped_divide(mC, (1, 4))\n",
    "    print(f\"[DSL INFO] Tiled Tensors:\")\n",
    "    print(f\"[DSL INFO] gA = {gA}\")\n",
    "    print(f\"[DSL INFO] gB = {gB}\")\n",
    "    print(f\"[DSL INFO] gC = {gC}\")\n",
    "    \n",
    "    vector_elementwise_add_kernel(gA, gB, gC).launch(\n",
    "        grid=(cute.size(gC, mode=[1]) // threads_per_block, 1, 1),\n",
    "        block=(threads_per_block, 1, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe58a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DSL INFO] Tiled Tensors:\n",
      "[DSL INFO] gA = tensor<ptr<f32, gmem, align<16>> o ((1,4),(40960,1024)):((0,1),(4096,4))>\n",
      "[DSL INFO] gB = tensor<ptr<f32, gmem, align<16>> o ((1,4),(40960,1024)):((0,1),(4096,4))>\n",
      "[DSL INFO] gC = tensor<ptr<f32, gmem, align<16>> o ((1,4),(40960,1024)):((0,1),(4096,4))>\n",
      "THread domain m=40960 , n=1024\n",
      "[DSL INFO] sliced gA = tensor<ptr<f32, gmem, align<16>> o ((1,4)):((0,1))>\n",
      "[DSL INFO] sliced gB = tensor<ptr<f32, gmem, align<16>> o ((1,4)):((0,1))>\n",
      "Average execution time: 0.4593ms\n",
      "Performance (GFLOPS): 365.2569GFLOPS\n",
      "Effective Memory Bandwidth: 4383.08 GB/s\n"
     ]
    }
   ],
   "source": [
    "M,N= 40960,4096\n",
    "a = torch.randn(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "b = torch.randn(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "c = torch.zeros(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "a_ = from_dlpack(a, assumed_align=16)\n",
    "b_ = from_dlpack(b, assumed_align=16)\n",
    "c_ = from_dlpack(c, assumed_align=16)\n",
    "\n",
    "compiled_func = cute.compile(vectorized_elementwise_add, a_, b_, c_)\n",
    "compiled_func(a_, b_, c_)\n",
    "\n",
    "# verify correctness\n",
    "torch.testing.assert_close(c, a + b)\n",
    "\n",
    "benchmark(partial(compiled_func, a_, b_, c_), num_warmups=5, num_iters=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
