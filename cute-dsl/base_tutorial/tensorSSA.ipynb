{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecfbce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cutlass\n",
    "import cutlass.cute as cute\n",
    "from cutlass.cute.runtime import from_dlpack\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52bae78",
   "metadata": {},
   "source": [
    "# Introduction to the TensorSSA in CuTe DSL\n",
    "\n",
    "This tutorial introduces what is the `TensorSSA` and why we need it. We also give some examples to show how to use `TensorSSA`.\n",
    "\n",
    "## What is TensorSSA\n",
    "\n",
    "`TensorSSA` is a Python class that represents a tensor value in Static Single Assignment (SSA) form within the CuTe DSL. You can think of it as a tensor residing in a (simulated) register.\n",
    "\n",
    "## Why TensorSSA\n",
    "\n",
    "`TensorSSA` encapsulates the underlying MLIR tensor value into an object that's easier to manipulate in Python. By overloading numerous Python operators (like `+`, `-`, `*`, `/`, `[]`, etc.), it allows users to express tensor computations (primarily element-wise operations and reductions) in a more Pythonic way. These element-wise operations are then translated into optimized vectorization instructions.\n",
    "\n",
    "It's part of the CuTe DSL, serving as a bridge between the user-described computational logic and the lower-level MLIR IR, particularly for representing and manipulating register-level data.\n",
    "\n",
    "## When to use TensorSSA\n",
    "\n",
    "`TensorSSA` is primarily used in the following scenarios:\n",
    "  在 CuTe DSL 中，TensorSSA 是**“寄存器中的张量”**。它与普通的 Tensor 有本质区别：\n",
    "  - Tensor：是一个视图 (View)。它包含一个指向内存（全局内存、共享内存或寄存器堆）的指针和一个布局 (Layout)。它描述了数据“在哪里”以及“如何排列”。\n",
    "  - TensorSSA：是一个值 (Value)。它代表已经被加载到硬件寄存器中的实际数值，采用静态单赋值 (Static Single Assignment, SSA) 形式。它是不可变的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49420673",
   "metadata": {},
   "source": [
    "### Load from memory and store to memory\n",
    "\n",
    "：a.load() 在底层触发了 MLIR 的 vector.load 指令。TensorSSA 内部包裹了一个 MLIR 的 vector 类型值。这保证了后续的所有运算都是在硬件的向量单元上直接执行，而不是逐元素循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6fd8665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_vec: tensor_value<vector<12xf32> o (3, 4)>\n",
      "b_vec: tensor_value<vector<12xf32> o (3, 4)>\n",
      "tensor(raw_ptr(0x00007ffe8f9f44c0: f32, rmem, align<32>) o (3,4):(1,3), data=\n",
      "       [[ 2.000000,  2.000000,  2.000000,  2.000000, ],\n",
      "        [ 2.000000,  2.000000,  2.000000,  2.000000, ],\n",
      "        [ 2.000000,  2.000000,  2.000000,  2.000000, ]])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def load_and_store(res: cute.Tensor, a: cute.Tensor, b: cute.Tensor):\n",
    "    \"\"\"\n",
    "    Load data from memory and store the result to memory.\n",
    "\n",
    "    :param res: The destination tensor to store the result.\n",
    "    :param a: The source tensor to be loaded.\n",
    "    :param b: The source tensor to be loaded.\n",
    "    \"\"\"\n",
    "    a_vec = a.load()   #! def load(self, *, loc=None, ip=None) -> \"TensorSSA\": ...\n",
    "                       #* a 是 Tensor (内存)，a_vec 是 TensorSSA (寄存器) \n",
    "                       #* TensorSSA 内部包裹了一个 MLIR 的 vector 类型值。这保证了后续的所有运算都是在硬件的向量单元上直接执行，而不是逐元素循环。\n",
    "    print(f\"a_vec: {a_vec}\")\n",
    "    b_vec = b.load()\n",
    "    print(f\"b_vec: {b_vec}\") \n",
    "    res.store(a_vec + b_vec)\n",
    "    cute.print_tensor(a_vec + b_vec)\n",
    "\n",
    "a = np.ones(12).reshape((3, 4)).astype(np.float32)\n",
    "b = np.ones(12).reshape((3, 4)).astype(np.float32)\n",
    "c = np.zeros(12).reshape((3, 4)).astype(np.float32)\n",
    "load_and_store(from_dlpack(c), from_dlpack(a), from_dlpack(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7030fd",
   "metadata": {},
   "source": [
    "### Register-Level Tensor Operations\n",
    "\n",
    "When writing kernel logic, various computations, transformations, slicing, etc., are performed on data loaded into registers.\n",
    "\n",
    "底层逻辑：它使用了 MLIR 的 vector.extract_strided_slice。由于 CuTe 是列优先 (Column-Major) 而 MLIR 向量通常期望行优先，TensorSSA 会自动处理数据的 Shuffle (洗牌) 操作，确保逻辑上的切片在物理寄存器中正确实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fbf6be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_value<vector<24xf32> o (4, 2, 3)> -> tensor_value<vector<12xf32> o (4, 3)>\n",
      "tensor(raw_ptr(0x000055fc790cbc40: f32, generic, align<4>) o (4,3):(3,1), data=\n",
      "       [[ 3.000000,  4.000000,  5.000000, ],\n",
      "        [ 9.000000,  10.000000,  11.000000, ],\n",
      "        [ 15.000000,  16.000000,  17.000000, ],\n",
      "        [ 21.000000,  22.000000,  23.000000, ]])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def apply_slice(src: cute.Tensor, dst: cute.Tensor, indices: cutlass.Constexpr):\n",
    "    \"\"\"\n",
    "    Apply slice operation on the src tensor and store the result to the dst tensor.\n",
    "\n",
    "    :param src: The source tensor to be sliced.\n",
    "    :param dst: The destination tensor to store the result.\n",
    "    :param indices: The indices to slice the source tensor.\n",
    "    \"\"\"\n",
    "    src_vec = src.load()\n",
    "    dst_vec = src_vec[indices]\n",
    "    print(f\"{src_vec} -> {dst_vec}\")\n",
    "    if cutlass.const_expr(isinstance(dst_vec, cute.TensorSSA)):\n",
    "        dst.store(dst_vec)\n",
    "        cute.print_tensor(dst)\n",
    "    else:\n",
    "        dst[0] = dst_vec\n",
    "        cute.print_tensor(dst)\n",
    "\n",
    "#   内存 (src)  ──load()──▶  寄存器 (src_vec: TensorSSA)\n",
    "#                                 │\n",
    "#                           [indices] 切片\n",
    "#                                 │\n",
    "#                                 ▼\n",
    "#                           dst_vec (TensorSSA 或 标量)\n",
    "#                                 │\n",
    "#                 ┌───────────────┴───────────────┐\n",
    "#                 │                               │\n",
    "#           TensorSSA?                        标量?\n",
    "#                 │                               │\n",
    "#          dst.store(dst_vec)              dst[0] = dst_vec\n",
    "#                 │                               │\n",
    "#                 └───────────────┬───────────────┘\n",
    "#                                 ▼\n",
    "#                           内存 (dst)\n",
    "\n",
    "\n",
    "\n",
    "def slice_1():\n",
    "    src_shape = (4, 2, 3)\n",
    "    dst_shape = (4, 3)\n",
    "    indices = (None, 1, None)    #! None 在 CuTe DSL 中等价于通配符 _（保留该维度所有元素）\n",
    "\n",
    "    a = np.arange(np.prod(src_shape)).reshape(*src_shape).astype(np.float32)\n",
    "    dst = np.random.randn(*dst_shape).astype(np.float32)\n",
    "    apply_slice(from_dlpack(a), from_dlpack(dst), indices)\n",
    "\n",
    "slice_1()\n",
    "\n",
    "\n",
    "#   ┌─────────────────────────────────────────────────────────────────┐\n",
    "#   │  NumPy (CPU)                                                    │\n",
    "#   │  a: shape (4,2,3), values 0-23                                  │\n",
    "#   └───────────────────────────┬─────────────────────────────────────┘\n",
    "#                               │ from_dlpack()\n",
    "#                               ▼\n",
    "#   ┌─────────────────────────────────────────────────────────────────┐\n",
    "#   │  CuTe Tensor (cute.Tensor)                                      │\n",
    "#   │  内存指针 + Layout 元数据                                         │\n",
    "#   └───────────────────────────┬─────────────────────────────────────┘\n",
    "#                               │ src.load()\n",
    "#                               ▼\n",
    "#   ┌─────────────────────────────────────────────────────────────────┐\n",
    "#   │  TensorSSA (register-level)                                     │\n",
    "#   │  src_vec: vector<24xf32> o (4,2,3)                              │\n",
    "#   └───────────────────────────┬─────────────────────────────────────┘\n",
    "#                               │ src_vec[(None, 1, None)]\n",
    "#                               ▼\n",
    "#   ┌─────────────────────────────────────────────────────────────────┐\n",
    "#   │  TensorSSA (sliced)                                             │\n",
    "#   │  dst_vec: vector<12xf32> o (4,3)                                │\n",
    "#   │  值: [3,4,5, 9,10,11, 15,16,17, 21,22,23]                        │\n",
    "#   └───────────────────────────┬─────────────────────────────────────┘\n",
    "#                               │ dst.store(dst_vec)\n",
    "#                               ▼\n",
    "#   ┌─────────────────────────────────────────────────────────────────┐\n",
    "#   │  结果写回内存                                                    │\n",
    "#   └─────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "#   切片操作提取的是原张量中 Dim 1 = 1 的那一\"层\"（即每个 4 个 2×3 子矩阵的第 2 行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc96b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_value<vector<24xf32> o (4, 2, 3)> -> ?\n",
      "tensor(raw_ptr(0x000055fc71cef030: f32, generic, align<4>) o (1):(1), data=\n",
      "       [ 13.000000, ])\n"
     ]
    }
   ],
   "source": [
    "def slice_2():\n",
    "    src_shape = (4, 2, 3)\n",
    "    dst_shape = (1,)\n",
    "    indices = 10\n",
    "    a = np.arange(np.prod(src_shape)).reshape(*src_shape).astype(np.float32)\n",
    "    dst  = np.random.randn(*dst_shape).astype(np.float32)\n",
    "    apply_slice(from_dlpack(a), from_dlpack(dst), indices)\n",
    "\n",
    "slice_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918c117",
   "metadata": {},
   "source": [
    "#### cute.print_tensor，只能打印 HBM 中的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "053f4cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t[1,2] = 5.000000\n",
      "tensor(raw_ptr(0x00007ffe8f9f44e0: f32, rmem, align<32>) o (4):(3), data=\n",
      "       [ 1.000000, ],\n",
      "       [ 4.000000, ],\n",
      "       [ 7.000000, ],\n",
      "       [ 10.000000, ])\n",
      "tensor(raw_ptr(0x00007ffe8f9f44c0: f32, rmem, align<32>) o (3):(1), data=\n",
      "       [ 6.000000, ],\n",
      "       [ 7.000000, ],\n",
      "       [ 8.000000, ])\n",
      "t[2] = 6.000000 (equivalent to t[(2,0)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "@cute.jit\n",
    "def slicing_examples(t: cute.Tensor):\n",
    "    # Scalar access\n",
    "    cute.printf(\"t[1,2] = {}\", t[1, 2])\n",
    "\n",
    "    #! 提取第二列 (shape: (N,)) using (None, row_index)\n",
    "    row = t[(None, 1)]\n",
    "    row_frag = cute.make_rmem_tensor(row.layout, row.element_type)\n",
    "    row_frag.store(row.load())      #! 需要读回HBM才能通过 cute.print_tensor\n",
    "    # print(\"Second row:\")\n",
    "    cute.print_tensor(row_frag)\n",
    "\n",
    "    #!  提取第三行 (shape: (M,)) using (col_index, None)\n",
    "    col = t[(2, None)]\n",
    "    col_frag = cute.make_rmem_tensor(col.layout, col.element_type)\n",
    "    col_frag.store(col.load())\n",
    "    # print(\"Third column:\")\n",
    "    cute.print_tensor(col_frag)\n",
    "\n",
    "    # Printing the first row directly (*t[2] == *t[2, 0])\n",
    "    cute.printf(\n",
    "        \"t[2] = {} (equivalent to t[{}])\",\n",
    "        t[2],\n",
    "        cute.make_identity_tensor(t.layout.shape)[2]\n",
    "    )\n",
    "\n",
    "# 4x3 example tensor\n",
    "arr = torch.arange(12, dtype=torch.float32).reshape(4, 3)\n",
    "slicing_examples(from_dlpack(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbd35c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(raw_ptr(0x00007ffe8f9f44e0: bf16, rmem, align<32>) o (4,3):(1,4), data=\n",
      "       [[ 0.000000, -0.000000, -16.000000, ],\n",
      "        [ 0.000000,  293601280.000000, -81857426827086135296.000000, ],\n",
      "        [ 186091919409888222206532988439248240640.000000,  34634616274944.000000,  190079603397242969825244409620089274368.000000, ],\n",
      "        [ 0.000000,  0.000000,  0.000000, ]])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def create_tensor_exam():\n",
    "    layout = cute.make_layout((4, 3))\n",
    "    # ptr = cute.make_ptr()\n",
    "    reg_tensor = cute.make_rmem_tensor(layout, cutlass.BFloat16)\n",
    "    #reg_tensor.fill(0)\n",
    "    cute.print_tensor(reg_tensor)\n",
    "    # return tensor\n",
    "\n",
    "create_tensor_exam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d91056",
   "metadata": {},
   "source": [
    "## 算术运算 (Arithmetic) —— 自动向量化与类型提升\n",
    "\n",
    "  add_res = a_vec + b_vec # 两个寄存器向量相加\n",
    "  mul_res = a_vec * 2.0   # 向量与标量相乘\n",
    "  - 含义：TensorSSA 重载了 Python 运算符，使其表现得像 NumPy。\n",
    "  - 底层逻辑：\n",
    "    - 自动广播：当 a_vec * 2.0 执行时，TensorSSA 会自动调用 vector.broadcast 将标量 2.0 扩展为与 a_vec 相同形状的向量。\n",
    "    - 类型提升：它会自动处理不同精度（如 Float16 到 Float32）的转换，生成对应的类型转换 IR 指令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bf22697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(raw_ptr(0x00007ffe8f9f4420: f32, rmem, align<32>) o (3):(1), data=\n",
      "       [ 3.000000, ],\n",
      "       [ 3.000000, ],\n",
      "       [ 3.000000, ])\n",
      "tensor(raw_ptr(0x00007ffe8f9f4440: f32, rmem, align<32>) o (3):(1), data=\n",
      "       [-1.000000, ],\n",
      "       [-1.000000, ],\n",
      "       [-1.000000, ])\n",
      "tensor(raw_ptr(0x00007ffe8f9f4460: f32, rmem, align<32>) o (3):(1), data=\n",
      "       [ 2.000000, ],\n",
      "       [ 2.000000, ],\n",
      "       [ 2.000000, ])\n",
      "tensor(raw_ptr(0x00007ffe8f9f4480: f32, rmem, align<32>) o (3):(1), data=\n",
      "       [ 0.500000, ],\n",
      "       [ 0.500000, ],\n",
      "       [ 0.500000, ])\n",
      "tensor(raw_ptr(0x00007ffe8f9f44a0: f32, rmem, align<32>) o (3):(1), data=\n",
      "       [ 0.000000, ],\n",
      "       [ 0.000000, ],\n",
      "       [ 0.000000, ])\n",
      "tensor(raw_ptr(0x00007ffe8f9f44c0: f32, rmem, align<32>) o (3):(1), data=\n",
      "       [ 1.000000, ],\n",
      "       [ 1.000000, ],\n",
      "       [ 1.000000, ])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def binary_op_2(res: cute.Tensor, a: cute.Tensor, c: cutlass.Constexpr):\n",
    "    a_vec = a.load()\n",
    "\n",
    "    add_res = a_vec + c\n",
    "    cute.print_tensor(add_res)  # prints [3.000000, 3.000000, 3.000000]\n",
    "\n",
    "    sub_res = a_vec - c\n",
    "    cute.print_tensor(sub_res)  # prints [-1.000000, -1.000000, -1.000000]\n",
    "\n",
    "    mul_res = a_vec * c\n",
    "    cute.print_tensor(mul_res)  # prints [2.000000, 2.000000, 2.000000]\n",
    "\n",
    "    div_res = a_vec / c\n",
    "    cute.print_tensor(div_res)  # prints [0.500000, 0.500000, 0.500000]\n",
    "\n",
    "    floor_div_res = a_vec // c\n",
    "    cute.print_tensor(floor_div_res)  # prints [0.000000, 0.000000, 0.000000]\n",
    "\n",
    "    mod_res = a_vec % c\n",
    "    cute.print_tensor(mod_res)  # prints [1.000000, 1.000000, 1.000000]\n",
    "\n",
    "\n",
    "a = np.empty((3,), dtype=np.float32)\n",
    "a.fill(1.0)\n",
    "c = 2.0\n",
    "res = np.empty((3,), dtype=np.float32)\n",
    "binary_op_2(from_dlpack(res), from_dlpack(a), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ed5a204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False]\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def binary_op_3(res: cute.Tensor, a: cute.Tensor, b: cute.Tensor):\n",
    "    a_vec = a.load()\n",
    "    b_vec = b.load()\n",
    "\n",
    "    gt_res = a_vec > b_vec\n",
    "    res.store(gt_res)\n",
    "\n",
    "    \"\"\"\n",
    "    ge_res = a_ >= b_   # [False, True, False]\n",
    "    lt_res = a_ < b_    # [True, False, True]\n",
    "    le_res = a_ <= b_   # [True, False, True]\n",
    "    eq_res = a_ == b_   # [False, False, False]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "a = np.array([1, 2, 3], dtype=np.float32)\n",
    "b = np.array([2, 1, 4], dtype=np.float32)\n",
    "res = np.empty((3,), dtype=np.bool_)\n",
    "binary_op_3(from_dlpack(res), from_dlpack(a), from_dlpack(b))\n",
    "print(res)  # prints [False, True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a6a598",
   "metadata": {},
   "source": [
    "## SSA reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89db59c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.000000\n",
      "tensor(raw_ptr(0x00007ffe8f9f4460: f16, rmem, align<32>) o (2):(1), data=\n",
      "       [ 6.000000, ],\n",
      "       [ 15.000000, ])\n",
      "tensor(raw_ptr(0x00007ffe8f9f4480: f16, rmem, align<32>) o (3):(1), data=\n",
      "       [ 5.000000, ],\n",
      "       [ 7.000000, ],\n",
      "       [ 9.000000, ])\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def ssa_reduce(a: cute.Tensor):\n",
    "    \"\"\"\n",
    "    Apply reduction operation on the src tensor.\n",
    "\n",
    "    :param src: The source tensor to be reduced.\n",
    "    \"\"\"\n",
    "    a_vec = a.load()\n",
    "    red_res = a_vec.reduce(cute.ReductionOp.ADD, 0.0, reduction_profile = 0)\n",
    "    cute.printf(red_res)\n",
    "\n",
    "    red_res = a_vec.reduce(cute.ReductionOp.ADD, 0.0, reduction_profile = (None, 1))\n",
    "    cute.print_tensor(red_res)\n",
    "\n",
    "    red_res = a_vec.reduce(cute.ReductionOp.ADD, 0.0, reduction_profile = (1, None))\n",
    "    cute.print_tensor(red_res)\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float16)\n",
    "ssa_reduce(from_dlpack(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc09f9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
