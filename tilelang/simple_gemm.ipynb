{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529fda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tilelang\n",
    "import tilelang.language as T\n",
    "from tilelang.intrinsics import make_mma_swizzle_layout as make_swizzle_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae53779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tilelang.language.proxy.BufferProxy at 0x7f16813866c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb031c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(M, N, K, block_M, block_N, block_K, num_stages=3, dtype=\"float16\", accum_dtype=\"float\"):\n",
    "    \"\"\"\n",
    "    TileLang GEMM kernel factory function.\n",
    "    Based on official tilelang documentation examples.\n",
    "    \"\"\"\n",
    "    @T.prim_func\n",
    "    def main(\n",
    "        A: T.Tensor((M, K), dtype),  # FIX: T.Buffer -> T.Tensor\n",
    "        B: T.Tensor((K, N), dtype),  # FIX: T.Buffer -> T.Tensor\n",
    "        C: T.Tensor((M, N), dtype),  # FIX: T.Buffer -> T.Tensor\n",
    "    ):\n",
    "        # FIX: T.kernel -> T.Kernel, thread -> threads\n",
    "        with T.Kernel(T.ceildiv(N, block_N), T.ceildiv(M, block_M), threads=128) as (bx, by):\n",
    "\n",
    "            A_shared = T.alloc_shared((block_M, block_K), dtype)\n",
    "            B_shared = T.alloc_shared((block_K, block_N), dtype)\n",
    "            C_local  = T.alloc_fragment((block_M, block_N), accum_dtype)\n",
    "\n",
    "            # Clear local accumulation\n",
    "            T.clear(C_local)\n",
    "\n",
    "            for ko in T.Pipelined(T.ceildiv(K, block_K), num_stages=num_stages):\n",
    "                # Copy A tile to shared memory\n",
    "                T.copy(A[by * block_M, ko * block_K], A_shared)\n",
    "                # Copy B tile to shared memory\n",
    "                T.copy(B[ko * block_K, bx * block_N], B_shared)\n",
    "                # Perform matrix multiplication (FIX: removed k_pack parameter)\n",
    "                T.gemm(A_shared, B_shared, C_local)\n",
    "            \n",
    "            # Copy result from local fragment to global memory\n",
    "            T.copy(C_local, C[by * block_M, bx * block_N])\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0d7df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configurations: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_M</th>\n",
       "      <th>block_N</th>\n",
       "      <th>block_K</th>\n",
       "      <th>num_stages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    block_M  block_N  block_K  num_stages\n",
       "0        64       64       32           2\n",
       "1        64       64       32           3\n",
       "2        64       64       64           2\n",
       "3        64       64       64           3\n",
       "4        64      128       32           2\n",
       "5        64      128       32           3\n",
       "6        64      128       64           2\n",
       "7        64      128       64           3\n",
       "8       128       64       32           2\n",
       "9       128       64       32           3\n",
       "10      128       64       64           2\n",
       "11      128       64       64           3\n",
       "12      128      128       32           2\n",
       "13      128      128       32           3\n",
       "14      128      128       64           2\n",
       "15      128      128       64           3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def build_gemm_config():\n",
    "    \"\"\"Generate configurations for autotuning GEMM kernel.\"\"\"\n",
    "    block_M = [64, 128]      # Reduced for faster testing\n",
    "    block_N = [64, 128]\n",
    "    block_K = [32, 64]\n",
    "    num_stages = [2, 3]\n",
    "\n",
    "    all_combinations = itertools.product(\n",
    "        block_M, block_N, block_K, num_stages\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"block_M\": m, \"block_N\": n, \"block_K\": k,\n",
    "            \"num_stages\": s\n",
    "        }\n",
    "        for m, n, k, s in all_combinations\n",
    "    ]\n",
    "\n",
    "configs = build_gemm_config()\n",
    "df = pd.DataFrame(configs)\n",
    "print(f\"Total configurations: {len(configs)}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e6284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-14 05:45:53,444 INFO:Auto-tuning with 0.9 CPU utilizations, 180 CPUs available, 162 CPUs will be used\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3ae2162e974f488020492e6e57042d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compiling configurations:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[-1]`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:45:59  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
      "2026-01-14 05:46:00  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ff8015c26b423e8adce31db3110db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Bench configurations:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Latency 2.7071359157562256 with config {'block_M': 64, 'block_N': 64, 'block_K': 32, 'num_stages': 2} at index 0\n",
      "2026-01-14 05:46:05,739 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 1.7991039752960205 with config {'block_M': 64, 'block_N': 128, 'block_K': 32, 'num_stages': 2} at index 1\n",
      "2026-01-14 05:46:06,084 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 1.283136010169983 with config {'block_M': 64, 'block_N': 128, 'block_K': 32, 'num_stages': 3} at index 2\n",
      "2026-01-14 05:46:06,427 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 1.2653759717941284 with config {'block_M': 64, 'block_N': 64, 'block_K': 64, 'num_stages': 3} at index 3\n",
      "2026-01-14 05:46:06,764 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 1.0901119709014893 with config {'block_M': 128, 'block_N': 64, 'block_K': 64, 'num_stages': 2} at index 4\n",
      "2026-01-14 05:46:07,099 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 1.8329919576644897 with config {'block_M': 128, 'block_N': 64, 'block_K': 32, 'num_stages': 2} at index 5\n",
      "2026-01-14 05:46:07,440 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 0.799232006072998 with config {'block_M': 64, 'block_N': 128, 'block_K': 64, 'num_stages': 3} at index 6\n",
      "2026-01-14 05:46:07,783 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 0.7958080172538757 with config {'block_M': 128, 'block_N': 64, 'block_K': 64, 'num_stages': 3} at index 7\n",
      "2026-01-14 05:46:08,117 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 1.2582080364227295 with config {'block_M': 128, 'block_N': 64, 'block_K': 32, 'num_stages': 3} at index 8\n",
      "2026-01-14 05:46:08,471 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 1.1073919534683228 with config {'block_M': 64, 'block_N': 128, 'block_K': 64, 'num_stages': 2} at index 9\n",
      "2026-01-14 05:46:08,811 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 2.101151943206787 with config {'block_M': 64, 'block_N': 64, 'block_K': 32, 'num_stages': 3} at index 10\n",
      "2026-01-14 05:46:09,154 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 0.756928026676178 with config {'block_M': 128, 'block_N': 128, 'block_K': 32, 'num_stages': 3} at index 11\n",
      "2026-01-14 05:46:09,510 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 1.06985604763031 with config {'block_M': 128, 'block_N': 128, 'block_K': 32, 'num_stages': 2} at index 12\n",
      "2026-01-14 05:46:09,856 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 0.531391978263855 with config {'block_M': 128, 'block_N': 128, 'block_K': 64, 'num_stages': 3} at index 13\n",
      "2026-01-14 05:46:10,211 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 1.6432960033416748 with config {'block_M': 64, 'block_N': 64, 'block_K': 64, 'num_stages': 2} at index 14\n",
      "2026-01-14 05:46:10,541 WARNING:\n",
      "Incompatible input tensor properties detected between cached tensors and tensors regenerated for the current configuration trial. This can happen if different tuning configurations require different input shapes/dtypes and input tensor caching is enabled.\n",
      "To ensure fresh, compatible inputs are generated for every trial you can disable caching by setting:\n",
      "  `cache_input_tensors=False`\n",
      "within your `.set_compile_args(...)` call.\n",
      "\n",
      "Tuned Latency 0.6958400011062622 with config {'block_M': 128, 'block_N': 128, 'block_K': 64, 'num_stages': 2} at index 15\n",
      "Best config: {'block_M': 128, 'block_N': 128, 'block_K': 64, 'num_stages': 3}\n",
      "Best latency: 0.5314 ms\n",
      "Max error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "from tilelang.autotuner import AutoTuner\n",
    "from tilelang.autotuner.capture import set_autotune_inputs\n",
    "\n",
    "device = th.device(\"cuda\")\n",
    "M, N, K = 8192, 4096, 4096\n",
    "A = th.randn((M, K), dtype=th.float16, device=device)\n",
    "B = th.randn((K, N), dtype=th.float16, device=device)\n",
    "C = th.empty((M, N), dtype=th.float16, device=device)\n",
    "\n",
    "# Kernel entrypoint - parameter names must match config keys exactly\n",
    "def kernel_entrypoint(block_M=None, block_N=None, block_K=None, num_stages=None):\n",
    "    return matmul(M, N, K, block_M, block_N, block_K, num_stages)\n",
    "\n",
    "with set_autotune_inputs(A, B, C):\n",
    "    autotuner = AutoTuner.from_kernel(\n",
    "        kernel=kernel_entrypoint,\n",
    "        configs=build_gemm_config()\n",
    "    ).set_compile_args(\n",
    "        out_idx=[-1],\n",
    "        target=\"auto\"\n",
    "    )\n",
    "\n",
    "    result = autotuner.run(warmup=10, rep=50)\n",
    "\n",
    "print(f\"Best config: {result.config}\")\n",
    "print(f\"Best latency: {result.latency:.4f} ms\")\n",
    "\n",
    "best_kernel = result.kernel\n",
    "C_output = best_kernel(A, B)\n",
    "\n",
    "# Verify correctness\n",
    "C_ref = A @ B\n",
    "print(f\"Max error: {(C_output - C_ref).abs().max().item():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
